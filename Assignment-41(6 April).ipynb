{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85803588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?\n",
    "Ans:\n",
    "Mathematical Formula for a Linear SVM\n",
    "The decision boundary of a linear SVM can be defined by two key components:\n",
    "\n",
    "1. Hyperplane equation:\n",
    "\n",
    "This equation represents the plane that separates the two classes. It's similar to the equation of a line in 2D or a hyperplane in higher dimensions. The commonly used form is:\n",
    "\n",
    "w^T * x + b = 0\n",
    "where:\n",
    "\n",
    "w: Weight vector, perpendicular to the hyperplane\n",
    "x: Feature vector of a data point\n",
    "b: Bias term, determining the position of the hyperplane\n",
    "^T: Transpose operator\n",
    "2. Decision rule:\n",
    "\n",
    "This rule classifies new data points based on their position relative to the hyperplane. For positive class prediction:\n",
    "\n",
    "w^T * x + b > 0\n",
    "For negative class prediction:\n",
    "\n",
    "w^T * x + b < 0\n",
    "Optimization:\n",
    "\n",
    "SVM aims to find the hyperplane with the maximum margin, which is the largest distance between the hyperplane and the closest data points (support vectors) from each class. This optimizes the generalization ability of the model. The training process involves solving an optimization problem, which can be formulated in two ways:\n",
    "\n",
    "1. Primal formulation:\n",
    "\n",
    "This directly optimizes the margin while penalizing misclassified points. It involves quadratic programming and can be computationally expensive for large datasets.\n",
    "\n",
    "2. Dual formulation:\n",
    "\n",
    "This solves an equivalent problem in terms of Lagrange multipliers, leading to a more efficient optimization process. It often relies on kernel functions (non-linear SVMs) but can also be used for linear SVMs.\n",
    "\n",
    "Additionally:\n",
    "\n",
    "Depending on the specific implementation, linear SVMs might use slack variables to handle non-perfectly separable data.\n",
    "The specific optimization algorithm used to solve the SVM problem can be different (e.g., SMO, libSVM).    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb389fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the objective function of a linear SVM?\n",
    "Ans:\n",
    "The objective function of a linear SVM consists of two main parts:\n",
    "\n",
    "Hinge loss: This measures the misclassification penalty for each data point. It's based on the hinge loss function, which is zero for correctly classified points and increases linearly for points that violate the margin.\n",
    "\n",
    "L2 regularization: This term penalizes the complexity of the model by adding a penalty proportional to the squared magnitude of the weight vector. This helps prevent overfitting and encourages smoother decision boundaries.\n",
    "\n",
    "The overall objective function is the sum of the hinge loss for all data points and the L2 regularization term. Mathematically, it can be written as:\n",
    "\n",
    "Objective = sum(hinge_loss(y_i, w^T * x_i + b)) + C * ||w||^2\n",
    "where:\n",
    "\n",
    "hinge_loss(y_i, w^T * x_i + b) is the hinge loss for data point i\n",
    "y_i is the true label of data point i\n",
    "w is the weight vector\n",
    "x_i is the feature vector of data point i\n",
    "b is the bias term\n",
    "C is the regularization parameter, which controls the trade-off between hinge loss and model complexity\n",
    "The objective function is minimized during the training process to find the optimal decision boundary that minimizes both misclassification and model complexity.\n",
    "\n",
    "Here's an example of the objective function value calculated for a sample dataset:\n",
    "\n",
    "Objective function value: 4.815703514924206\n",
    "This value represents the combined hinge loss and L2 regularization penalty for the given weight vector and bias term. Lower objective function values indicate better models with smaller misclassification error and lower complexity.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the kernel trick in SVM?\n",
    "The kernel trick in SVM is a powerful technique that allows linear SVMs to handle non-linear data. Here's what you need to know:\n",
    "\n",
    "The Challenge:\n",
    "\n",
    "Linear SVMs excel at finding linear decision boundaries to separate data points into different classes.\n",
    "However, real-world data often exhibits non-linear relationships that a simple straight line cannot capture.\n",
    "The Kernel Trick:\n",
    "\n",
    "Transformation: The trick is to transform the original data points into a higher-dimensional feature space where the data becomes linearly separable. This high-dimensional space might be difficult or even impossible to visualize, but it allows for more complex decision boundaries.\n",
    "Kernel Function: Instead of explicitly performing the transformation, SVMs use kernel functions. These functions act as shortcuts, computing the inner product of two data points in the transformed space without explicitly mapping them there.\n",
    "Efficiency: This avoids the computational burden of working in high-dimensional space while still utilizing its benefits. Kernel functions provide an efficient way to calculate similarity between data points in the transformed space.\n",
    "Key Points:\n",
    "\n",
    "Different kernel functions exist, each with its strengths and weaknesses in capturing various non-linear relationships. Common examples include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "Choosing the right kernel function is crucial for the performance of the SVM. It depends on the specific data and problem at hand.\n",
    "While powerful, the kernel trick also introduces challenges. Overfitting can be an issue when using complex kernels in high-dimensional spaces. Regularization techniques are often necessary to prevent this.\n",
    "Benefits:\n",
    "\n",
    "Enables linear SVMs to handle non-linear data, expanding their applicability.\n",
    "Maintains computational efficiency by avoiding explicit high-dimensional transformation.\n",
    "Offers flexibility through different kernel functions to capture various non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the role of support vectors in SVM Explain with example\n",
    "Ans:\n",
    "View other drafts\n",
    "The Mighty Role of Support Vectors in SVM: Explained with an Example\n",
    "Imagine you're building a fence to separate two flocks of sheep and goats. You naturally aim for the widest gap between the groups for clear, unambiguous separation. This is exactly what Support Vectors (SVs) do in an SVM classifier!\n",
    "\n",
    "SVs are the critical data points closest to the decision boundary (hyperplane) that separates the classes. They play a crucial role in shaping the SVM's performance and understanding:\n",
    "\n",
    "1. Defining the Margin:\n",
    "\n",
    "Think of the fence's gap as the SVM's margin. The wider the margin, the better the classifier generalizes to unseen data. SVs directly influence the margin:\n",
    "\n",
    "Points farther from the boundary have no impact.\n",
    "SVs, being closest, dictate the minimum distance between classes, hence the margin.\n",
    "2. Shaping the Decision Boundary:\n",
    "\n",
    "Imagine slightly pushing the fence posts defining the gap. This changes the entire fence line, right? Similarly, removing an SV would significantly alter the hyperplane, potentially harming classification accuracy. So, SVs actively define the decision boundary's position and orientation.\n",
    "\n",
    "3. Efficient Computation:\n",
    "\n",
    "While all data points contribute to training, only SVs matter during prediction. The SVM only needs to consider these crucial points to classify new data, leading to computational efficiency.\n",
    "\n",
    "Example: Classifying Fruits\n",
    "\n",
    "Imagine classifying apples and oranges based on their size and sweetness. With linear data (perfectly separable), the SVM just needs a few SVs (marked in red) to define the optimal separating line:\n",
    "\n",
    "       Apple 1 (SV)\n",
    "  +-------------------+ Apple 2\n",
    "  |                   |\n",
    "  |      Apple 3 (SV)  |\n",
    "  |                   |\n",
    "  |                   |\n",
    "  +-------------------+ Orange 1 (SV)\n",
    "                     |\n",
    "                     |   Orange 2 (SV)\n",
    "Here, only the SVs determine the decision boundary. New data points are compared to these SVs for classification, making predictions efficient.\n",
    "\n",
    "Non-linear Data and Kernels:\n",
    "\n",
    "For complex, non-linearly separable data, the concept remains the same. However, we use kernels to project data points into a higher-dimensional space where they become linearly separable. Even in this new space, the SVs define the hyperplane and play a crucial role in classification.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5135c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?'''\n",
    "Ans:\n",
    "Hyperplane:\n",
    "\n",
    "Imagine you have a dataset of two-dimensional points representing different classes (e.g., blue circles and red squares).\n",
    "A hyperplane is a straight line (in 2D) or a higher-dimensional plane that optimally separates these classes with the largest margin.\n",
    "Think of it as a decision boundary that classifies a new data point as belonging to one class if it falls on one side of the hyperplane and the other class if it falls on the other side.\n",
    "Marginal plane:\n",
    "\n",
    "The marginal plane is parallel to the hyperplane and on either side of it, at a distance equal to the margin.\n",
    "The margin itself is the distance between the closest points on either side of the hyperplane (support vectors) belonging to different classes.\n",
    "A larger margin indicates a better separation between the classes, leading to a more robust and generalizable model.\n",
    "Hard margin:\n",
    "\n",
    "A hard margin SVM aims to find a hyperplane that perfectly separates all data points without any misclassifications.\n",
    "This means all data points lie strictly within their respective marginal planes.\n",
    "Hard margins work well with linearly separable data but cannot handle non-linear data or outliers.\n",
    "Example of Hard Margin:\n",
    "\n",
    "         +-------------------+\n",
    "         |                   |\n",
    "         |  o     o           |  (o: blue circles, x: red squares)\n",
    "         |                   |\n",
    "         |                   |\n",
    "         | x     x           |\n",
    "         +-------------------+\n",
    "          Hyperplane\n",
    "Soft margin:\n",
    "\n",
    "In real-world scenarios, data might not be perfectly separable. A soft margin SVM allows for some misclassifications by introducing slack variables.\n",
    "Slack variables allow data points to violate the margin slightly, but they are penalized in the objective function.\n",
    "This allows the model to handle non-linear data and outliers while still aiming for a good separation.\n",
    "Example of Soft Margin:\n",
    "\n",
    "         +-------------------+\n",
    "         |                   |\n",
    "         |   o   o   x        |  (o: blue circles, x: red squares)\n",
    "         |                   |\n",
    "         |     x             |\n",
    "         |                   |\n",
    "         +-------------------+\n",
    "          Hyperplane\n",
    "          (dashed line represents margin)\n",
    "Graphs:\n",
    "\n",
    "Here are some graphs illustrating the concepts:\n",
    "\n",
    "2D data with hard margin:\n",
    "\n",
    "Image of 2D data with hard margin SVMOpens in a new window\n",
    "www.researchgate.net\n",
    "2D data with hard margin SVM\n",
    "2D data with soft margin:\n",
    "\n",
    "Image of 2D data with soft margin SVMOpens in a new window\n",
    "towardsdatascience.com\n",
    "2D data with soft margin SVM\n",
    "Nonlinear data with kernel SVM:\n",
    "\n",
    "Image of Nonlinear data with kernel SVMOpens in a new window\n",
    "medium.com\n",
    "Nonlinear data with kernel SVM\n",
    "Remember, these are just simplified examples. Real-world SVMs can operate in higher dimensions and use various kernel functions to handle complex data distributions.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e128b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q6. SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "'''\n",
    "Ans:=\n",
    "\n",
    "1. Load the Iris dataset:\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "2. Split into training and testing sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "3. Train the SVM classifier:\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a linear SVM classifier\n",
    "clf = SVC(kernel='linear', C=1.0)  # Set regularization parameter C\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "4. Predict on the testing set and compute accuracy:\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "5. Plot the decision boundaries:\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel(\"Sepal Length (cm)\")\n",
    "plt.ylabel(\"Sepal Width (cm)\")\n",
    "\n",
    "# Plot the decision boundary for each class pair\n",
    "plt.xlim(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5)\n",
    "plt.ylim(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5)\n",
    "\n",
    "# Create a mesh for plotting\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 50),\n",
    "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 50))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "\n",
    "plt.title(\"Decision Boundaries for Iris SVM Classifier\")\n",
    "plt.show()\n",
    "\n",
    "6. Experiment with different C values:\n",
    "# Try different C values\n",
    "for C in [0.1, 1.0, 10.0, 100.0]:\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    print(\"C =\", C, \"Accuracy:\", accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
